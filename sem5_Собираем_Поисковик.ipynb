{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 5    \n",
    "## Собираем поисковик \n",
    "\n",
    "![](https://bilimfili.com/wp-content/uploads/2017/06/bir-urune-emek-vermek-o-urune-olan-deger-algimizi-degistirir-mi-bilimfilicom.jpg) \n",
    "\n",
    "\n",
    "Мы уже все знаем, для того чтобы сделать поисковик. Осталось соединить все части вместе.    \n",
    "Итак, для поисковика нам понадобятся:         \n",
    "**1. База документов **\n",
    "> в первом дз - корпус Друзей    \n",
    "в сегодняшнем дз - корпус юридических вопросов-ответов    \n",
    "в итоговом проекте - корпус Авито   \n",
    "\n",
    "**2. Функция индексации**                 \n",
    "Что делает: собирает информацию о корпусе, по которуму будет происходить поиск      \n",
    "Своя для каждого поискового метода:       \n",
    "> A. для обратного индекса она создает обратный индекс (чудо) и сохраняет статистики корпуса, необходимые для Okapi BM25 (средняя длина документа в коллекции, количество доков ... )             \n",
    "> B. для поиска через word2vec эта функция создает вектор для каждого документа в коллекции путем, например, усреднения всех векторов коллекции       \n",
    "> C. для поиска через doc2vec эта функция создает вектор для каждого документа               \n",
    "\n",
    "   Не забывайте сохранить все, что насчитает эта функция. Если это будет происходить налету во время поиска, понятно, что он будет работать сто лет     \n",
    "   \n",
    "**3. Функция поиска**     \n",
    "Можно разделить на две части:\n",
    "1. функция вычисления близости между запросом и документом    \n",
    "> 1. для индекса это Okapi BM25\n",
    "> 2. для w2v и d2v это обычная косинусная близость между векторами          \n",
    "2. ранжирование (или просто сортировка)\n",
    "\n",
    "\n",
    "Время все это реализовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Индексация\n",
    "## Word2Vec\n",
    "### Задание 1\n",
    "Загрузите любую понравившуюся вам word2vec модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import gensim\n",
    "from judicial_splitter import split_paragraph, get_sentences\n",
    "import string\n",
    "import json\n",
    "import os\n",
    "from gensim import matutils\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если модель без тэгов\n",
    "model = Word2Vec.load('/Users/Stoneberry/Desktop/Uni/Прога/4 курс/Поиск/araneum_none_fasttextcbow_300_5_2018/araneum_none_fasttextcbow_300_5_2018.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если модель с POS-тэггингом\n",
    "model2 = KeyedVectors.load_word2vec_format('/Users/Stoneberry/Desktop/Uni/Прога/4 курс/Поиск/taiga_upos_skipgram_300_2_2018.vec', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2 \n",
    "Напишите функцию индексации для поиска через word2vec. Она должна для каждого документа из корпуса строить вектор.   \n",
    "Все вектора надо сохранить, по формату советую json. При сохранении не забывайте, что вам надо сохранить не только  вектор, но и опознователь текста, которому он принадлежит. \n",
    "Для поисковика это может быть url страницы, для поиска по текстовому корпусу сам текст.\n",
    "\n",
    "> В качестве документа для word2vec берите **параграфы** исходного текста, а не весь текст целиком. Так вектора будут более осмысленными. В противном случае можно получить один очень общий вектор, релевантый совершенно разным запросам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_stopwords = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делим тексты на параграфы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting(main_dir, stopwords={}, del_stop=True):\n",
    "    for root, dirs, files in os.walk(main_dir):\n",
    "        for name in files:\n",
    "            if not '.DS_Store' in name:\n",
    "                with open(os.path.join(root, name), 'r', encoding='utf-8') as f:\n",
    "                    text = f.read() \n",
    "                    parts = split_paragraph(get_sentences(text), 4)\n",
    "                    for part in parts:\n",
    "                        part_proc = preprocessing(part, stopwords, del_stopwords=del_stop, del_digit=True)\n",
    "                        yield (part_proc, name, part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessing(input_text, stopwords={}, del_stopwords=True, del_digit=True):\n",
    "    \"\"\"\n",
    "    :input: raw text\n",
    "        1. lowercase, del punctuation, tokenize\n",
    "        2. normal form\n",
    "        3. del stopwords\n",
    "        4. del digits\n",
    "    :return: lemmas\n",
    "    \"\"\"\n",
    "    words = [x.lower().strip(string.punctuation+'»«–…') for x in word_tokenize(input_text)]\n",
    "    lemmas = [mystem.lemmatize(x)[0] for x in words if x]\n",
    "\n",
    "    lemmas_arr = []\n",
    "    for lemma in lemmas:\n",
    "        if del_stopwords:\n",
    "            if lemma in stopwords:\n",
    "                continue\n",
    "        if del_digit:\n",
    "            if lemma.isdigit():\n",
    "                continue\n",
    "        lemmas_arr.append(lemma)\n",
    "    return lemmas_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если нет слова в модели, то возврашается пустой массив\n",
    "\n",
    "def get_w2v_vectors(text, model):\n",
    "    \"\"\"\n",
    "    Получает вектор документа\n",
    "    Не работает\n",
    "    \"\"\"\n",
    "    vec = 0\n",
    "    lenght = 0\n",
    "    \n",
    "    for word in text:\n",
    "        try: \n",
    "            vec += model.wv[word]\n",
    "            lenght += 1\n",
    "        except: None\n",
    "    \n",
    "    if lenght != 0:\n",
    "        return vec / lenght\n",
    "    return [0] * 300\n",
    "\n",
    "\n",
    "def get_w2v_vectors(lemmas, model): \n",
    "    \"\"\"\n",
    "    Получает вектор документа\n",
    "    Работает\n",
    "    \"\"\"\n",
    "    lemmas_vectors = []\n",
    "    for lemma in lemmas:\n",
    "        try:\n",
    "            lemmas_vectors.append(model.wv[lemma])\n",
    "        except:\n",
    "            None\n",
    "    if lemmas_vectors:\n",
    "        doc_vec = sum(lemmas_vectors)\n",
    "        normalized_vec = matutils.unitvec(doc_vec)\n",
    "        return list(normalized_vec)\n",
    "    else: \n",
    "        return [0] * 300\n",
    "\n",
    "\n",
    "def save_w2v_base(main_dir, model, stopwords={}, del_stop=True):\n",
    "    \"\"\"Индексирует всю базу для поиска через word2vec\"\"\"\n",
    "    \n",
    "    all_data = []\n",
    "\n",
    "    for part in splitting(main_dir, stopwords=stopwords, del_stop=del_stop):\n",
    "        vec_info = {}\n",
    "        vec_info['id'] = part[1]\n",
    "        vec_info['text'] = part[2]\n",
    "        vec_info['vec'] = get_w2v_vectors(part[0], model)\n",
    "        all_data.append(vec_info)\n",
    "    return all_data #json.dumps(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = '/Users/Stoneberry/Desktop/Uni/Прога/4 курс/Поиск/лекции/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = save_w2v_base(main_dir, model, stopwords=russian_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec\n",
    "### Задание 3\n",
    "Напишите функцию обучения doc2vec на юридических текстах, и получите свою кастомную d2v модель. \n",
    "> Совет: есть мнение, что для обучения doc2vec модели не нужно удалять стоп-слова из корпуса. Они являются важными семантическими элементами.      \n",
    "\n",
    "Важно! В качестве документа для doc2vec берите **параграфы** исходного текста, а не весь текст целиком. И не забывайте про предобработку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagged_data_creator(main_dir, stopwords={}, del_stop=False):\n",
    "\n",
    "    tagged_data = []\n",
    "    i = 0\n",
    "    for part in splitting(main_dir, stopwords=stopwords, del_stop=del_stop):\n",
    "        tagged_data.append(TaggedDocument(words=part[0], tags=[i]))\n",
    "        #print('Complited: ' + str(part[2]) + ' '  + part[1])\n",
    "        i += 1\n",
    "    return tagged_data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doc2vec(tagged_data):\n",
    "    \n",
    "    model = Doc2Vec(vector_size=100, min_count=5, alpha=0.025, \n",
    "                min_alpha=0.025, epochs=100, workers=2, dm=1)\n",
    "    \n",
    "    print('building vocabulary')\n",
    "    model.build_vocab(tagged_data)\n",
    "    print('starting training...')\n",
    "    model.random.seed(42)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    print('model is trained')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building vocabulary\n",
      "starting training...\n",
      "model is trained\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "model_data = tagged_data_creator('/Users/Stoneberry/Desktop/Uni/Прога/4 курс/Поиск/лекции/train', stopwords=russian_stopwords, del_stop=False)\n",
    "with open('tagged_data.pickle', 'wb') as f:\n",
    "    pickle.dump(model_data , f)\n",
    "\n",
    "my_model = train_doc2vec(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4\n",
    "Напишите функцию индексации для поиска через doc2vec. Она должна для каждого документа из корпуса получать вектор.    \n",
    "Все вектора надо сохранить, по формату советую json. При сохранении не забывайте, что вам надо сохранить не только вектор, но и опознователь текста, которому он принадлежит. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_d2v_vectors(model, text):\n",
    "    \"\"\"Получает вектор документа\"\"\"\n",
    "    return model.infer_vector(text)\n",
    "    \n",
    "\n",
    "def save_d2v_base(main_dir, model, stopwords={}, del_stop=True):\n",
    "    \"\"\"Индексирует всю базу для поиска через doc2vec\"\"\"\n",
    "    \n",
    "    all_data = []\n",
    "\n",
    "    for part in splitting(main_dir, stopwords=stopwords, del_stop=del_stop):\n",
    "        vec_info = {}\n",
    "        vec_info['id'] = part[1]\n",
    "        vec_info['text'] = part[2]\n",
    "        vec_info['vec'] = get_d2v_vectors(model, part[0])\n",
    "        all_data.append(vec_info)\n",
    "    return all_data #json.dumps(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = save_d2v_base(main_dir, my_model, stopwords=russian_stopwords, del_stop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция поиска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обратного индекса функцией поиска является Okapi BM25. Она у вас уже должна быть реализована."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция измерения близости между векторами нам пригодится:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(v1, v2):\n",
    "    v1_norm = matutils.unitvec(np.array(v1))\n",
    "    v2_norm = matutils.unitvec(np.array(v2))\n",
    "    return np.dot(v1_norm, v2_norm)\n",
    "\n",
    "\n",
    "def similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5\n",
    "Напишите функцию для поиска через word2vec и для поиска через doc2vec, которая по входящему запросу выдает отсортированную выдачу документов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def culc_sim_score(data1, vec):\n",
    "    \n",
    "    \"\"\"\n",
    "    соединяю параграфы в текст\n",
    "    \"\"\"\n",
    "    \n",
    "    answer = defaultdict(list) # id : [text, sim_score]\n",
    "    \n",
    "    for part in data1:\n",
    "        obj = answer[part['id']]\n",
    "        sim = similarity(part['vec'], vec)\n",
    "        if obj == []:\n",
    "            obj.append('')\n",
    "            obj.append(float('-inf'))\n",
    "        obj[0] += part['text'] + ' '\n",
    "        if sim > obj[1]:\n",
    "            obj[1] = sim\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_w2v(string, model, data1, stopwords={}, amount=10, del_stop=True):\n",
    "    \n",
    "    if not isinstance(string, str):\n",
    "        raise ValueError('enter correct data')\n",
    "    \n",
    "    words = preprocessing(string, stopwords=stopwords, del_stopwords=del_stop, del_digit=True)\n",
    "    vec = get_w2v_vectors(words, model)\n",
    "    answer = culc_sim_score(data1, vec)\n",
    "    \n",
    "    for index, ans in enumerate(sorted(answer.items(), key=lambda x: x[1][1], reverse=True)):\n",
    "        if index >= amount: break\n",
    "        yield (ans[0], ans[1][0], ans[1][1])\n",
    "\n",
    "\n",
    "def search_d2v(string, model, data2, stopwords={}, del_stop=False, amount=10):\n",
    "    \n",
    "    if not isinstance(string, str):\n",
    "        raise ValueError('enter correct data')\n",
    "    \n",
    "    words = preprocessing(string, stopwords=stopwords, del_stopwords=del_stop, del_digit=True)\n",
    "    vec = get_d2v_vectors(model, words)\n",
    "    answer = culc_sim_score(data2, vec)\n",
    "    \n",
    "    for index, ans in enumerate(sorted(answer.items(), key=lambda x: x[1][1], reverse=True)):\n",
    "        if index >= amount: break\n",
    "        yield (ans[0], ans[1][0], ans[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99998.txt 0.3184982\n",
      "4.txt 0.31830204\n",
      "2.txt 0.31830204\n",
      "3.txt 0.3129466\n",
      "6.txt 0.3129466\n"
     ]
    }
   ],
   "source": [
    "for i in search_w2v('уговор', model, data1, del_stop=True, stopwords=russian_stopwords, amount=5):\n",
    "    print(i[0], i[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999.txt 0.1100643\n",
      "99998.txt 0.105139665\n",
      "99879.txt 0.098483324\n",
      "1.txt 0.08321444\n",
      "5.txt 0.056647025\n"
     ]
    }
   ],
   "source": [
    "for i in search_d2v('уговор', my_model, data2, del_stop=False, stopwords=russian_stopwords, amount=5):\n",
    "    print(i[0], i[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После выполнения всех этих заданий ваш поисковик готов, поздравляю!                  \n",
    "Осталось завернуть все написанное в питон скрипт, и сделать общую функцию поиска гибким, чтобы мы могли искать как по обратному индексу, так и по word2vec, так и по doc2vec.          \n",
    "Сделать это можно очень просто через старый добрый ``` if ```, который будет дергать ту или иную функцию поиска:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter(main_dir, del_stop=True, stopwords={}):\n",
    "\n",
    "    word_count = defaultdict(dict) # word : {id, count}\n",
    "    id_text = defaultdict(list) # id : [len, text]\n",
    "\n",
    "    for root, dirs, files in os.walk(main_dir):\n",
    "        for name in files:\n",
    "            if not '.DS_Store' in name:\n",
    "                with open(os.path.join(root, name), 'r', encoding='utf-8') as f:\n",
    "                    text = f.read() \n",
    "                    words = preprocessing(text, stopwords=stopwords, del_stopwords=del_stop, del_digit=True)\n",
    "                    id_text[name] += [len(words), text]\n",
    "                    prob = Counter(words)\n",
    "                    for word in prob:\n",
    "                        word_count[word][name] = prob[word]\n",
    "                \n",
    "    return word_count, id_text\n",
    "\n",
    "\n",
    "def score_BM25(qf, dl, avgdl, k1, b, N, n):\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "\n",
    "    qf - кол - во вхождений слова в документе\n",
    "    dl - длина документа\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    tf = qf / dl\n",
    "    idf = log(N - n + 0.5 / n + 0.5)\n",
    "    a = (k1 + 1) * tf\n",
    "    b = tf + k1*(1 - b + b*(dl / avgdl))\n",
    "\n",
    "    return (a / b) * idf\n",
    "\n",
    "\n",
    "def compute_sim(text, doc, id_text, word_count, N, stopwords={}, del_stop=True):\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError('enter correct data')\n",
    "    if not isinstance(doc, str):\n",
    "        raise ValueError('enter correct data')\n",
    "\n",
    "    opr = [' ', '  ', '\\t', '\\n']\n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    avgdl = np.mean([i[0] for i in id_text.values()])\n",
    "\n",
    "    ans = 0\n",
    "\n",
    "    words = preprocessing(text, stopwords=stopwords, del_stopwords=del_stop, del_digit=True)\n",
    "\n",
    "    for word in words:\n",
    "        if word in word_count:\n",
    "            if doc in word_count[word]:\n",
    "                qf = word_count[word][doc]\n",
    "            else:\n",
    "                qf = 0\n",
    "            dl = id_text[doc][0]\n",
    "            n = len(word_count[word])\n",
    "            ans += score_BM25(qf, dl, avgdl, k1, b, N, n)\n",
    "\n",
    "    return ans\n",
    "\n",
    "\n",
    "def get_search_result(text, id_text, word_count, stopwords={}, del_stop=True, amount=10):\n",
    "    \"\"\"\n",
    "    Compute sim score between search query and all documents in collection\n",
    "    Collect as pair (doc_id, score)\n",
    "    :param query: input text\n",
    "    :return: list of lists with (doc_id, score)\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError\n",
    "\n",
    "    arr = []\n",
    "    N = len(id_text)\n",
    "   \n",
    "    for doc in id_text:\n",
    "        arr.append((doc, id_text[doc][1], compute_sim(text, doc, id_text, word_count, N, stopwords=stopwords, del_stop=del_stop)))\n",
    "    \n",
    "    arr = sorted(arr, key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    return arr[:amount]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count, id_text = counter(main_dir, stopwords=russian_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.txt 0\n",
      "99997.txt 0\n",
      "99998.txt 0\n",
      "4.txt 0\n",
      "99999.txt 0\n"
     ]
    }
   ],
   "source": [
    "for i in get_search_result('уговор', id_text, word_count, stopwords=russian_stopwords, amount=5):\n",
    "    print(i[0], i[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как этого слова нет в той части текстов, на которой я обучала"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.txt 0.03491414211676592\n",
      "2.txt 0.02815044770681393\n",
      "99998.txt 0.02262174018738026\n",
      "3.txt 0.022539850630834624\n",
      "99997.txt 0.018944873627858447\n"
     ]
    }
   ],
   "source": [
    "for i in get_search_result('суд', id_text, word_count, stopwords=russian_stopwords, amount=5):\n",
    "    print(i[0], i[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(string, search_method, amount=10, del_stop=False, stopwords={}):\n",
    "\n",
    "    if search_method == 'inverted_index':\n",
    "        global word_count, id_text\n",
    "        search_result = get_search_result(string, id_text, word_count, del_stop=del_stop, stopwords=stopwords, amount=amount)\n",
    "\n",
    "    elif search_method == 'word2vec':\n",
    "        global model, data1\n",
    "        #model = Word2Vec.load('/Users/Stoneberry/Desktop/Uni/Прога/4 курс/Поиск/araneum_none_fasttextcbow_300_5_2018/araneum_none_fasttextcbow_300_5_2018.model')\n",
    "        #data = save_w2v_base(main_dir, model, stopwords=stopwords, del_stop=del_stop)\n",
    "        search_result = [i for i in search_w2v(string, model, data1, stopwords=stopwords, del_stop=del_stop, amount=amount)]\n",
    "\n",
    "    elif search_method == 'doc2vec':\n",
    "        global my_model, data2\n",
    "        #data = save_d2v_base(main_dir, my_model, stopwords=stopwords, del_stop=del_stop)\n",
    "        search_result = [i for i in search_d2v(string, my_model, data2, del_stop=del_stop, stopwords=stopwords, amount=amount)]\n",
    "\n",
    "    else:\n",
    "        raise TypeError('unsupported search method')\n",
    "    \n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.txt 0.6987984\n",
      "6.txt 0.6987984\n",
      "4.txt 0.6973086\n",
      "2.txt 0.6973086\n",
      "99879.txt 0.64392644\n"
     ]
    }
   ],
   "source": [
    "for i in search('суд', 'word2vec', amount=5, del_stop=False, stopwords=russian_stopwords):\n",
    "    print(i[0], i[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99998.txt 13.215175\n",
      "99879.txt 11.782065\n",
      "99999.txt 11.243983\n",
      "99997.txt 10.303209\n",
      "2.txt 10.15325\n"
     ]
    }
   ],
   "source": [
    "for i in search('суд', 'doc2vec', amount=5, del_stop=False, stopwords=russian_stopwords):\n",
    "    print(i[0], i[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.txt 0.03491414211676592\n",
      "2.txt 0.02815044770681393\n",
      "99998.txt 0.02262174018738026\n",
      "3.txt 0.022539850630834624\n",
      "99997.txt 0.018944873627858447\n"
     ]
    }
   ],
   "source": [
    "for i in search('суд', 'inverted_index', amount=5, del_stop=False, stopwords=russian_stopwords):\n",
    "    print(i[0], i[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
