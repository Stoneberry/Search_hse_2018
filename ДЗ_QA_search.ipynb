{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ДЗ по поиску\n",
    "\n",
    "Привет! Вам надо реализивать поисковик на базе вопросов-ответов с сайта [pravoved.ru](https://pravoved.ru/questions-archive/).        \n",
    "Поиск должен работать на трех технологиях:       \n",
    "1. обратном индексе     \n",
    "2. word2vec         \n",
    "3. doc2vec      \n",
    "\n",
    "Вы должны понять, какой метод и при каких условиях эксперимента на этом корпусе работает лучше.          \n",
    "Для измерения качества поиска найдите точность (accuracy) выпадания правильного ответа на конкретный вопрос (в этой базе у каждого вопроса есть только один правильный ответ). Точность нужно измерить для всей базы.    \n",
    "При этом давайте считать, что выпал правильный ответ, если он попал в **топ-5** поисковой выдачи.\n",
    "\n",
    "> Сделайте ваш поиск максимально качественным, чтобы значение точности стремилось к 1.     \n",
    "Для этого можно поэкспериментировать со следующим:       \n",
    "- модель word2vec (можно брать любую из опен сорса или обучить свою)\n",
    "- способ получения вектора документа через word2vec: простое среднее арифметическое или взвешивать каждый вектор в соответствии с его tf-idf      \n",
    "- количество эпох у doc2vec (начинайте от 100)\n",
    "- предобработка документов для обучения doc2vec (удалять / не удалять стоп-слова)\n",
    "- блендинг методов поиска: соединить результаты обратного индекса и w2v, или (что проще) w2v и d2v\n",
    "\n",
    "На это задание отведем 10 дней. Дэдлайн сдачи до полуночи 12.10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('qa_corpus.pkl', 'rb') as file:\n",
    "    qa_corpus = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всего в корпусе 1384 пары вопрос-ответ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1384"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qa_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняю ответы в отдельные файлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, item in enumerate(qa_corpus):\n",
    "    with open('/Users/Stoneberry/Desktop/Uni/Прога/4 курс/Поиск/лекции/qa_data/' + str(index) + '.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый элемент блока это вопрос, второй - ответ на него"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nДобрый день.Мой сын гражданин Украины (ДНР),имеет вид на жительство в Р.Ф., кот.получил проживая с 2014 г. в Нижегородской области.В 2017г. переехал на постоянное место жительство в г.Ростов.Официально трудоустроился на одно из промышл.предприятий г.Ростова.Оформил временную регистрацию в Ростове.В УФМС предупредили,что по истечении 90 дней он должен либо постоянно прописаться либо покинуть территорию России.Прошу проконсультировать как быть дальше.(Вернуться домой в Донецк,но здесь идет война,работы нет.В Ростове он работает по специальности.Он инженер машиностроитель.)Временная прописка до 15 марта.  Если он сможет приобрести какую либо недвижимость,как долго будет решаться вопрос о его постоянной прописке в Ростове.Как в этом случае будет решаться вопрос с видом на жительство в Ростове? Не получится ли ,что приобретя квартиру,он не успеет в ней прописаться до окончании срока временной регистрации. С уважением Людмила Евгеньевна.\\n',\n",
       " 'Добрый вечер!Из Вашего вопроса вообще ничего не ясно.Ваш сын по ВНЖ в Нижегородской обл. сделал временную\\xa0 на 90 дней в Ростове? Так? Или в чем заключается вопрос?С ув., АлёнаМиграционный юристРостов-на-Дону ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import gensim\n",
    "from judicial_splitter import split_paragraph, get_sentences\n",
    "import string\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from gensim import matutils\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_stopwords = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('/Users/Stoneberry/Desktop/Uni/Прога/4 курс/Поиск/araneum_none_fasttextcbow_300_5_2018/araneum_none_fasttextcbow_300_5_2018.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(input_text, stopwords={}, del_stopwords=True, del_digit=True):\n",
    "    \n",
    "    words = [x.lower().strip(string.punctuation+'»«–…') for x in word_tokenize(input_text)]\n",
    "    lemmas = [mystem.lemmatize(x)[0] for x in words if x]\n",
    "    lemmas_arr = []\n",
    "    for lemma in lemmas:\n",
    "        if del_stopwords:\n",
    "            if lemma in stopwords:\n",
    "                continue\n",
    "        if del_digit:\n",
    "            if lemma.isdigit():\n",
    "                continue\n",
    "        lemmas_arr.append(lemma)\n",
    "    return lemmas_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_vectors(lemmas, model): \n",
    "    #lemmas = preprocessing(query)\n",
    "    lemmas_vectors = []\n",
    "    for lemma in lemmas:\n",
    "        try:\n",
    "            lemmas_vectors.append(model.wv[lemma])\n",
    "        except:\n",
    "            None\n",
    "    if lemmas_vectors:\n",
    "        doc_vec = sum(lemmas_vectors)\n",
    "        normalized_vec = matutils.unitvec(doc_vec)\n",
    "        return list(normalized_vec)\n",
    "    else: \n",
    "        return [0] * 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я не понимаю, почему это не работает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_vectors2(text, model):\n",
    "    \"\"\"Получает вектор документа\"\"\"\n",
    "\n",
    "    vec = 0\n",
    "    lenght = 0\n",
    "    \n",
    "    for word in text:\n",
    "        try: \n",
    "            vec += model.wv[word]\n",
    "            lenght += 1\n",
    "        except: None\n",
    "    \n",
    "    if lenght != 0:\n",
    "        return vec / lenght\n",
    "    return [0] * 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting(main_dir, stopwords={}, del_stop=True):\n",
    "    for root, dirs, files in os.walk(main_dir):\n",
    "        for name in files:\n",
    "            if not '.DS_Store' in name:\n",
    "                with open(os.path.join(root, name), 'r', encoding='utf-8') as f:\n",
    "                    text = f.read() \n",
    "                    parts = split_paragraph(get_sentences(text), 4)\n",
    "                    for part in parts:\n",
    "                        part_proc = preprocessing(part, stopwords, del_stopwords=del_stop, del_digit=True)\n",
    "                        yield (part_proc, name, part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_w2v_base(main_dir, model, stopwords={}, del_stop=True):\n",
    "    \"\"\"Индексирует всю базу для поиска через word2vec\"\"\"\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    for part in splitting(main_dir, stopwords=stopwords, del_stop=del_stop):\n",
    "        vec_info = {}\n",
    "        vec_info['id'] = part[1]\n",
    "        vec_info['text'] = part[2]\n",
    "        vec_info['vec'] = get_w2v_vectors(part[0], model)\n",
    "        all_data.append(vec_info)\n",
    "\n",
    "    return all_data #json.dumps(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(v1, v2):\n",
    "    v1_norm = matutils.unitvec(np.array(v1))\n",
    "    v2_norm = matutils.unitvec(np.array(v2))\n",
    "    return np.dot(v1_norm, v2_norm)\n",
    "\n",
    "\n",
    "def similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2)\n",
    "\n",
    "\n",
    "def culc_sim_score(data1, vec):\n",
    "    \n",
    "    \"\"\"\n",
    "    соединяю параграфы в текст\n",
    "    \"\"\"\n",
    "    \n",
    "    answer = defaultdict(list) # id : [text, sim_score]\n",
    "    \n",
    "    for part in data1:\n",
    "        obj = answer[part['id']]\n",
    "        sim = similarity(part['vec'], vec)\n",
    "        if obj == []:\n",
    "            obj.append('')\n",
    "            obj.append(float('-inf'))\n",
    "        obj[0] += part['text'] + ' '\n",
    "        if sim > obj[1]:\n",
    "            obj[1] = sim\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_w2v(string, model, data1, stopwords={}, amount=10, del_stop=True):\n",
    "    \n",
    "    if not isinstance(string, str):\n",
    "        raise ValueError('enter correct data')\n",
    "    \n",
    "    words = preprocessing(string, stopwords=stopwords, del_stopwords=del_stop, del_digit=True)\n",
    "    vec = get_w2v_vectors(words, model)\n",
    "    answer = culc_sim_score(data1, vec)\n",
    "    \n",
    "    for index, ans in enumerate(sorted(answer.items(), key=lambda x: x[1][1], reverse=True)):\n",
    "        if index >= amount: break\n",
    "        yield (ans[0], ans[1][0], ans[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = '/Users/Stoneberry/Desktop/Uni/Прога/4 курс/Поиск/лекции/qa_data/'\n",
    "data1 = save_w2v_base(main_dir, model, stopwords=russian_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2774566473988439"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = 0\n",
    "for index, qa in enumerate(qa_corpus):\n",
    "    idx = str(index) + '.txt'\n",
    "    for i in search_w2v(qa[0], model, data1, stopwords=russian_stopwords, del_stop=True, amount=5):\n",
    "        if i[0] == idx:\n",
    "            pred += 1\n",
    "pred / len(qa_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.txt 0.766294\n",
      "315.txt 0.745872\n",
      "402.txt 0.7099507\n",
      "622.txt 0.68759716\n",
      "643.txt 0.63410246\n",
      "817.txt 0.6237143\n",
      "1307.txt 0.61546665\n",
      "647.txt 0.6010889\n",
      "1358.txt 0.6004466\n",
      "263.txt 0.60017496\n"
     ]
    }
   ],
   "source": [
    "for i in search_w2v(qa_corpus[35][0], model, data1, stopwords=russian_stopwords, amount=10):\n",
    "    print(i[0], i[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words={'тогда', 'свою', 'если', 'ж', 'этой', 'ей', 'после', 'хоть', 'опять', 'том', 'ни', 'не', 'все', 'даже', 'что', 'другой', 'его', 'бы', 'к', 'сам', 'нас', 'моя', 'тебя', 'им', 'ведь', 'вас', 'она', 'когда', 'три', 'на', 'хорошо', 'раз', 'потому', 'в', 'тут', 'здесь', 'я', 'нибудь', 'ее', '...оже', 'сейчас', 'их', 'при', 'будет', 'куда', 'этом', 'него', 'вам', 'ну', 'а', 'они', 'чтоб', 'во'},\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range = (1,1), stop_words=russian_stopwords)\n",
    "tfidf.fit([i[1] for i in qa_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_vectors_tf(lemmas, model, tfidf): \n",
    "    \n",
    "    vocab = tfidf.vocabulary_\n",
    "    arr = tfidf.transform([' '.join(lemmas)]).toarray()[0]\n",
    "    weight = 1\n",
    "    lemmas_vectors = []\n",
    "    all_weight = 0\n",
    "\n",
    "    for lemma in lemmas:\n",
    "        if lemma in vocab:\n",
    "            weight = arr[vocab[lemma]]\n",
    "        try:\n",
    "            lemmas_vectors.append(model.wv[lemma] * weight)\n",
    "            all_weight += weight\n",
    "        except:\n",
    "            None\n",
    "    if lemmas_vectors:\n",
    "        doc_vec = sum(lemmas_vectors) #/ all_weight\n",
    "        #doc_vec = list(matutils.unitvec(doc_vec))\n",
    "        return doc_vec\n",
    "    else: \n",
    "        return [0] * 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_w2v_tf(string, model, data1, stopwords={}, amount=10, del_stop=True):\n",
    "    \n",
    "    if not isinstance(string, str):\n",
    "        raise ValueError('enter correct data')\n",
    "        \n",
    "    global tfidf\n",
    "    words = preprocessing(string, stopwords=stopwords, del_stopwords=del_stop, del_digit=True)\n",
    "    vec = get_w2v_vectors_tf(words, model, tfidf)\n",
    "    answer = culc_sim_score(data1, vec)\n",
    "    \n",
    "    for index, ans in enumerate(sorted(answer.items(), key=lambda x: x[1][1],  reverse=True)):\n",
    "        if index >= amount: break\n",
    "        yield (ans[0], ans[1][0], ans[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25072254335260113"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = 0\n",
    "for index, qa in enumerate(qa_corpus):\n",
    "    idx = str(index) + '.txt'\n",
    "    for i in search_w2v_tf(qa[0], model, data1, stopwords=russian_stopwords, del_stop=True, amount=5):\n",
    "        if i[0] == idx:\n",
    "            pred += 1\n",
    "pred / len(qa_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расчет среднего арифметического оказался лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.txt 3.0969675\n",
      "315.txt 3.0812178\n",
      "402.txt 2.9510856\n",
      "622.txt 2.852767\n",
      "643.txt 2.4845889\n",
      "303.txt 2.4650326\n",
      "1259.txt 2.4629474\n",
      "817.txt 2.4600716\n",
      "1307.txt 2.4401164\n",
      "568.txt 2.4354715\n"
     ]
    }
   ],
   "source": [
    "for i in search_w2v_tf(qa_corpus[35][0], model, data1, stopwords=russian_stopwords, amount=10):\n",
    "    print(i[0], i[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество эпох у doc2vec (начинайте от 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagged_data_creator(main_dir, stopwords={}, del_stop=False):\n",
    "    \n",
    "    tagged_data = []\n",
    "    i = 0\n",
    "    for part in splitting(main_dir, stopwords=stopwords, del_stop=del_stop):\n",
    "        tagged_data.append(TaggedDocument(words=part[0], tags=[i]))\n",
    "        #print('Complited: ' + part[1])\n",
    "        i += 1\n",
    "    return tagged_data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doc2vec(tagged_data, epo=100):\n",
    "    \n",
    "    model = Doc2Vec(vector_size=100, min_count=5, alpha=0.025, \n",
    "                min_alpha=0.025, epochs=epo, workers=2, dm=1)\n",
    "    \n",
    "    print('building vocabulary')\n",
    "    model.build_vocab(tagged_data)\n",
    "    print('starting training...')\n",
    "    model.random.seed(42)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    print('model is trained')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_d2v_vectors(model, text):\n",
    "    \"\"\"Получает вектор документа\"\"\"\n",
    "    return model.infer_vector(text)\n",
    "    \n",
    "\n",
    "def save_d2v_base(main_dir, model, stopwords={}, del_stop=False):\n",
    "    \"\"\"Индексирует всю базу для поиска через doc2vec\"\"\"\n",
    "    \n",
    "    all_data = []\n",
    "\n",
    "    for part in splitting(main_dir, stopwords=stopwords, del_stop=del_stop):\n",
    "        vec_info = {}\n",
    "        vec_info['id'] = part[1]\n",
    "        vec_info['text'] = part[2]\n",
    "        vec_info['vec'] = get_d2v_vectors(model, part[0])\n",
    "        all_data.append(vec_info)\n",
    "    return all_data #json.dumps(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_d2v(string, model, data2, stopwords={}, del_stop=False, amount=10):\n",
    "    \n",
    "    if not isinstance(string, str):\n",
    "        raise ValueError('enter correct data')\n",
    "    \n",
    "    words = preprocessing(string, stopwords=stopwords, del_stopwords=del_stop, del_digit=True)\n",
    "    vec = get_d2v_vectors(model, words)\n",
    "    answer = culc_sim_score(data2, vec)\n",
    "    \n",
    "    for index, ans in enumerate(sorted(answer.items(), key=lambda x: x[1][1], reverse=True)):\n",
    "        if index >= amount: break\n",
    "        yield (ans[0], ans[1][0], ans[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir2 = '/Users/Stoneberry/Desktop/Uni/Прога/4 курс/Поиск/лекции/train'\n",
    "model_data = tagged_data_creator(dir2, stopwords=russian_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 100\n",
      "building vocabulary\n",
      "starting training...\n",
      "model is trained\n",
      "Starting: 200\n",
      "building vocabulary\n",
      "starting training...\n",
      "model is trained\n",
      "Starting: 300\n",
      "building vocabulary\n",
      "starting training...\n",
      "model is trained\n",
      "Starting: 400\n",
      "building vocabulary\n",
      "starting training...\n",
      "model is trained\n"
     ]
    }
   ],
   "source": [
    "for epo in range(100, 500, 100):\n",
    "\n",
    "    print('Starting: ' + str(epo))\n",
    "    my_model = train_doc2vec(model_data, epo=epo)\n",
    "    data2 = save_d2v_base(main_dir, my_model, stopwords=russian_stopwords, del_stop=True)\n",
    "\n",
    "    pred = 0\n",
    "    for index, qa in enumerate(qa_corpus):\n",
    "        idx = str(index) + '.txt'\n",
    "        for i in search_d2v(qa[0], my_model, data2, stopwords=russian_stopwords, del_stop=True, amount=5):\n",
    "            if i[0] == idx:\n",
    "                pred += 1\n",
    "    res.append(pred / len(qa_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f49b9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(100, 500, 100)), res)\n",
    "plt.ylabel('some numbers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший результат на 200 эпохах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработка документов для обучения doc2vec (удалять / не удалять стоп-слова)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не удаляя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building vocabulary\n",
      "starting training...\n",
      "model is trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1791907514450867"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = train_doc2vec(model_data, epo=100)\n",
    "data2 = save_d2v_base(main_dir, my_model, stopwords=russian_stopwords, del_stop=False)\n",
    "\n",
    "pred = 0\n",
    "for index, qa in enumerate(qa_corpus):\n",
    "    idx = str(index) + '.txt'\n",
    "    for i in search_d2v(qa[0], my_model, data2, stopwords=russian_stopwords, del_stop=False, amount=5):\n",
    "        if i[0] == idx:\n",
    "            pred += 1\n",
    "pred / len(qa_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаляя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1893063583815029"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = save_d2v_base(main_dir, my_model, stopwords=russian_stopwords, del_stop=True)\n",
    "pred = 0\n",
    "\n",
    "for index, qa in enumerate(qa_corpus):\n",
    "    idx = str(index) + '.txt'\n",
    "    for i in search_d2v(qa[0], my_model, data2, stopwords=russian_stopwords, del_stop=True, amount=5):\n",
    "        if i[0] == idx:\n",
    "            pred += 1\n",
    "pred / len(qa_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, удаляя стоп слова, accuracy выше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d2v & w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Блендинг методов поиска: соединить результаты обратного индекса и w2v, или (что проще) w2v и d2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging(w2v, d2v, all_):\n",
    "    \n",
    "    ans = {}\n",
    "\n",
    "    for item in all_:\n",
    "\n",
    "        if item in w2v: \n",
    "            it_w = w2v[item]\n",
    "        else: it_w = 0\n",
    "\n",
    "        if item in d2v: \n",
    "            it_d = d2v[item]\n",
    "        else: it_d = 0\n",
    "        \n",
    "        ans[item] = (it_w * 0.7 + it_d * 0.3) / 2\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serach_w2_d2(string, model, my_model, data1, data2, stopwords={}, del_stop=False, amount=10):\n",
    "\n",
    "    w2v = {i[0]:i[2] for i in search_w2v(string, model, data1, stopwords=stopwords, del_stop=del_stop, amount=amount)}\n",
    "    d2v = {i[0]:i[2] for i in search_d2v(string, my_model, data2, stopwords=stopwords, del_stop=del_stop, amount=amount)}\n",
    "    all_ = set(w2v) | set(d2v)\n",
    "    ans = merging(w2v, d2v, all_)\n",
    "    return sorted(ans.items(), reverse=True, key=lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3179190751445087"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = 0\n",
    "\n",
    "for index, qa in enumerate(qa_corpus):\n",
    "    idx = str(index) + '.txt'\n",
    "    for i in serach_w2_d2(qa[0], model, my_model, data1, data2,del_stop=True, stopwords=russian_stopwords, amount=5):\n",
    "        if i[0] == idx:\n",
    "            pred += 1\n",
    "pred / len(qa_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование двух моделей дает лучший результат"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter(main_dir, del_stop=True, stopwords={}):\n",
    "\n",
    "    word_count = defaultdict(dict) # word : {id, count}\n",
    "    id_text = defaultdict(list) # id : [len, text]\n",
    "\n",
    "    for root, dirs, files in os.walk(main_dir):\n",
    "        for name in files:\n",
    "            if not '.DS_Store' in name:\n",
    "                with open(os.path.join(root, name), 'r', encoding='utf-8') as f:\n",
    "                    text = f.read() \n",
    "                    words = preprocessing(text, stopwords=stopwords, del_stopwords=del_stop, del_digit=True)\n",
    "                    id_text[name] += [len(words), text]\n",
    "                    prob = Counter(words)\n",
    "                    for word in prob:\n",
    "                        word_count[word][name] = prob[word]\n",
    "                \n",
    "    return word_count, id_text\n",
    "\n",
    "\n",
    "def score_BM25(qf, dl, avgdl, k1, b, N, n):\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "\n",
    "    qf - кол - во вхождений слова в документе\n",
    "    dl - длина документа\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    tf = qf / dl\n",
    "    idf = log(N - n + 0.5 / n + 0.5)\n",
    "    a = (k1 + 1) * tf\n",
    "    b = tf + k1*(1 - b + b*(dl / avgdl))\n",
    "\n",
    "    return (a / b) * idf\n",
    "\n",
    "\n",
    "def compute_sim(text, doc, id_text, word_count, N, stopwords={}, del_stop=True):\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError('enter correct data')\n",
    "    if not isinstance(doc, str):\n",
    "        raise ValueError('enter correct data')\n",
    "\n",
    "    opr = [' ', '  ', '\\t', '\\n']\n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    avgdl = np.mean([i[0] for i in id_text.values()])\n",
    "\n",
    "    ans = 0\n",
    "\n",
    "    words = preprocessing(text, stopwords=stopwords, del_stopwords=del_stop, del_digit=True)\n",
    "\n",
    "    for word in words:\n",
    "        if word in word_count:\n",
    "            if doc in word_count[word]:\n",
    "                qf = word_count[word][doc]\n",
    "            else:\n",
    "                qf = 0\n",
    "            dl = id_text[doc][0]\n",
    "            n = len(word_count[word])\n",
    "            ans += score_BM25(qf, dl, avgdl, k1, b, N, n)\n",
    "\n",
    "    return ans\n",
    "\n",
    "\n",
    "def get_search_result(text, id_text, word_count, stopwords={}, del_stop=True, amount=10):\n",
    "    \"\"\"\n",
    "    Compute sim score between search query and all documents in collection\n",
    "    Collect as pair (doc_id, score)\n",
    "    :param query: input text\n",
    "    :return: list of lists with (doc_id, score)\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError\n",
    "\n",
    "    arr = []\n",
    "    N = len(id_text)\n",
    "   \n",
    "    for doc in id_text:\n",
    "        arr.append((doc, id_text[doc][1], compute_sim(text, doc, id_text, word_count, N, stopwords=stopwords, del_stop=del_stop)))\n",
    "    \n",
    "    arr = sorted(arr, key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    return arr[:amount]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count, id_text = counter(main_dir, stopwords=russian_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "687.txt 3.55123096620198\n",
      "329.txt 2.0828200173558153\n",
      "1008.txt 1.5150265255600737\n",
      "1176.txt 0.7071466707337445\n",
      "89.txt 0.5485908282723401\n"
     ]
    }
   ],
   "source": [
    "for i in get_search_result('приговор', id_text, word_count, stopwords=russian_stopwords, amount=5):\n",
    "    print(i[0], i[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Совмещение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging_all_3(w2v, d2v, okapi, all_):\n",
    "    \n",
    "    ans = {}\n",
    "\n",
    "    for item in all_:\n",
    "\n",
    "        if item in w2v: \n",
    "            it_w = w2v[item]\n",
    "        else: it_w = 0\n",
    "\n",
    "        if item in d2v: \n",
    "            it_d = d2v[item]\n",
    "        else: it_d = 0\n",
    "            \n",
    "        if item in okapi:\n",
    "            it_o = okapi[item]\n",
    "        else: it_o = 0\n",
    "        \n",
    "        ans[item] = (((it_w * 0.8 + it_o * 0.2) / 2) * 0.7 + it_d * 0.3) / 2\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serach_w2_d2_ok(string, model, my_model, data1, data2, word_count, id_text, stopwords={}, del_stop=False, amount=10):\n",
    "\n",
    "    w2v = {i[0]:i[2] for i in search_w2v(string, model, data1, stopwords=stopwords, del_stop=del_stop, amount=amount)}\n",
    "    d2v = {i[0]:i[2] for i in search_d2v(string, my_model, data2, stopwords=stopwords, del_stop=del_stop, amount=amount)}\n",
    "    okapi = {i[0]:i[2] for i in get_search_result(string, id_text, word_count, stopwords=russian_stopwords, del_stop=del_stop, amount=amount)}\n",
    "    all_ = set(w2v) | set(d2v) | set(okapi)\n",
    "    ans = merging_all_3(w2v, d2v, okapi, all_)\n",
    "    return sorted(ans.items(), reverse=True, key=lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3865606936416185"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = 0\n",
    "l = 0\n",
    "for index, qa in enumerate(qa_corpus):\n",
    "    l += 1\n",
    "    idx = str(index) + '.txt'\n",
    "    for i in serach_w2_d2_ok(qa[0], model, my_model, data1, data2, word_count, id_text, del_stop=True, stopwords=russian_stopwords, amount=5):\n",
    "        if i[0] == idx:\n",
    "            pred += 1\n",
    "pred / len(qa_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получается самый высокий результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
