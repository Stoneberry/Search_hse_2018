{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ДЗ по поиску\n",
    "\n",
    "Привет! Вам надо реализивать поисковик на базе вопросов-ответов с сайта [pravoved.ru](https://pravoved.ru/questions-archive/).        \n",
    "Поиск должен работать на трех технологиях:       \n",
    "1. обратном индексе     \n",
    "2. word2vec         \n",
    "3. doc2vec      \n",
    "\n",
    "Вы должны понять, какой метод и при каких условиях эксперимента на этом корпусе работает лучше.          \n",
    "Для измерения качества поиска найдите точность (accuracy) выпадания правильного ответа на конкретный вопрос (в этой базе у каждого вопроса есть только один правильный ответ). Точность нужно измерить для всей базы.    \n",
    "При этом давайте считать, что выпал правильный ответ, если он попал в **топ-5** поисковой выдачи.\n",
    "\n",
    "> Сделайте ваш поиск максимально качественным, чтобы значение точности стремилось к 1.     \n",
    "Для этого можно поэкспериментировать со следующим:       \n",
    "- модель word2vec (можно брать любую из опен сорса или обучить свою)\n",
    "- способ получения вектора документа через word2vec: простое среднее арифметическое или взвешивать каждый вектор в соответствии с его tf-idf      \n",
    "- количество эпох у doc2vec (начинайте от 100)\n",
    "- предобработка документов для обучения doc2vec (удалять / не удалять стоп-слова)\n",
    "- блендинг методов поиска: соединить результаты обратного индекса и w2v, или (что проще) w2v и d2v\n",
    "\n",
    "На это задание отведем 10 дней. Дэдлайн сдачи до полуночи 12.10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('qa_corpus.pkl', 'rb') as file:\n",
    "    qa_corpus = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всего в корпусе 1384 пары вопрос-ответ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1384"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qa_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняю ответы в отдельные файлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, item in enumerate(qa_corpus):\n",
    "    with open('/Users/Stoneberry/Desktop/Uni/Прога/4 курс/Поиск/лекции/qa_data/' + str(index) + '.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый элемент блока это вопрос, второй - ответ на него"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nДобрый день.Мой сын гражданин Украины (ДНР),имеет вид на жительство в Р.Ф., кот.получил проживая с 2014 г. в Нижегородской области.В 2017г. переехал на постоянное место жительство в г.Ростов.Официально трудоустроился на одно из промышл.предприятий г.Ростова.Оформил временную регистрацию в Ростове.В УФМС предупредили,что по истечении 90 дней он должен либо постоянно прописаться либо покинуть территорию России.Прошу проконсультировать как быть дальше.(Вернуться домой в Донецк,но здесь идет война,работы нет.В Ростове он работает по специальности.Он инженер машиностроитель.)Временная прописка до 15 марта.  Если он сможет приобрести какую либо недвижимость,как долго будет решаться вопрос о его постоянной прописке в Ростове.Как в этом случае будет решаться вопрос с видом на жительство в Ростове? Не получится ли ,что приобретя квартиру,он не успеет в ней прописаться до окончании срока временной регистрации. С уважением Людмила Евгеньевна.\\n',\n",
       " 'Добрый вечер!Из Вашего вопроса вообще ничего не ясно.Ваш сын по ВНЖ в Нижегородской обл. сделал временную\\xa0 на 90 дней в Ростове? Так? Или в чем заключается вопрос?С ув., АлёнаМиграционный юристРостов-на-Дону ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import gensim\n",
    "from judicial_splitter import split_paragraph, get_sentences\n",
    "import string\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from gensim import matutils\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_stopwords = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('/Users/Stoneberry/Desktop/Uni/Прога/4 курс/Поиск/araneum_none_fasttextcbow_300_5_2018/araneum_none_fasttextcbow_300_5_2018.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(input_text, stopwords={}, del_stopwords=True, del_digit=True):\n",
    "    \n",
    "    words = [x.lower().strip(string.punctuation+'»«–…') for x in word_tokenize(input_text)]\n",
    "    lemmas = [mystem.lemmatize(x)[0] for x in words if x]\n",
    "    lemmas_arr = []\n",
    "    for lemma in lemmas:\n",
    "        if del_stopwords:\n",
    "            if lemma in stopwords:\n",
    "                continue\n",
    "        if del_digit:\n",
    "            if lemma.isdigit():\n",
    "                continue\n",
    "        lemmas_arr.append(lemma)\n",
    "    return lemmas_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_vectors(lemmas, model): \n",
    "    #lemmas = preprocessing(query)\n",
    "    lemmas_vectors = []\n",
    "    for lemma in lemmas:\n",
    "        try:\n",
    "            lemmas_vectors.append(model.wv[lemma])\n",
    "        except:\n",
    "            None\n",
    "    if lemmas_vectors:\n",
    "        doc_vec = sum(lemmas_vectors)\n",
    "        normalized_vec = matutils.unitvec(doc_vec)\n",
    "        return list(normalized_vec)\n",
    "    else: \n",
    "        return [0] * 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я не понимаю, почему это не работает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_vectors2(text, model):\n",
    "    \"\"\"Получает вектор документа\"\"\"\n",
    "\n",
    "    vec = 0\n",
    "    lenght = 0\n",
    "    \n",
    "    for word in text:\n",
    "        try: \n",
    "            vec += model.wv[word]\n",
    "            lenght += 1\n",
    "        except: None\n",
    "    \n",
    "    if lenght != 0:\n",
    "        return vec / lenght\n",
    "    return [0] * 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting(main_dir, stopwords={}, del_stop=True):\n",
    "    for root, dirs, files in os.walk(main_dir):\n",
    "        for name in files:\n",
    "            if not '.DS_Store' in name:\n",
    "                with open(os.path.join(root, name), 'r', encoding='utf-8') as f:\n",
    "                    text = f.read() \n",
    "                    parts = split_paragraph(get_sentences(text), 4)\n",
    "                    for part in parts:\n",
    "                        part_proc = preprocessing(part, stopwords, del_stopwords=del_stop, del_digit=True)\n",
    "                        yield (part_proc, name, part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_w2v_base(main_dir, model, stopwords={}, del_stop=True):\n",
    "    \"\"\"Индексирует всю базу для поиска через word2vec\"\"\"\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    for part in splitting(main_dir, stopwords=stopwords, del_stop=del_stop):\n",
    "        vec_info = {}\n",
    "        vec_info['id'] = part[1]\n",
    "        vec_info['text'] = part[2]\n",
    "        vec_info['vec'] = get_w2v_vectors(part[0], model)\n",
    "        all_data.append(vec_info)\n",
    "\n",
    "    return all_data #json.dumps(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(v1, v2):\n",
    "    v1_norm = matutils.unitvec(np.array(v1))\n",
    "    v2_norm = matutils.unitvec(np.array(v2))\n",
    "    return np.dot(v1_norm, v2_norm)\n",
    "\n",
    "\n",
    "def similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2)\n",
    "\n",
    "\n",
    "def culc_sim_score(data1, vec):\n",
    "    \n",
    "    \"\"\"\n",
    "    соединяю параграфы в текст\n",
    "    \"\"\"\n",
    "    \n",
    "    answer = defaultdict(list) # id : [text, sim_score]\n",
    "    \n",
    "    for part in data1:\n",
    "        obj = answer[part['id']]\n",
    "        sim = similarity(part['vec'], vec)\n",
    "        if obj == []:\n",
    "            obj.append('')\n",
    "            obj.append(float('-inf'))\n",
    "        obj[0] += part['text'] + ' '\n",
    "        if sim > obj[1]:\n",
    "            obj[1] = sim\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_w2v(string, model, data1, stopwords={}, amount=10, del_stop=True):\n",
    "    \n",
    "    if not isinstance(string, str):\n",
    "        raise ValueError('enter correct data')\n",
    "    \n",
    "    words = preprocessing(string, stopwords=stopwords, del_stopwords=del_stop, del_digit=True)\n",
    "    vec = get_w2v_vectors(words, model)\n",
    "    answer = culc_sim_score(data1, vec)\n",
    "    \n",
    "    for index, ans in enumerate(sorted(answer.items(), key=lambda x: x[1][1], reverse=True)):\n",
    "        if index >= amount: break\n",
    "        yield (ans[0], ans[1][0], ans[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = '/Users/Stoneberry/Desktop/Uni/Прога/4 курс/Поиск/лекции/qa_data/'\n",
    "data1 = save_w2v_base(main_dir, model, stopwords=russian_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2774566473988439"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = 0\n",
    "for index, qa in enumerate(qa_corpus):\n",
    "    idx = str(index) + '.txt'\n",
    "    for i in search_w2v(qa[0], model, data1, stopwords=russian_stopwords, del_stop=True, amount=5):\n",
    "        if i[0] == idx:\n",
    "            pred += 1\n",
    "pred / len(qa_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.txt 0.766294\n",
      "315.txt 0.745872\n",
      "402.txt 0.7099507\n",
      "622.txt 0.68759716\n",
      "643.txt 0.63410246\n",
      "817.txt 0.6237143\n",
      "1307.txt 0.61546665\n",
      "647.txt 0.6010889\n",
      "1358.txt 0.6004466\n",
      "263.txt 0.60017496\n"
     ]
    }
   ],
   "source": [
    "for i in search_w2v(qa_corpus[35][0], model, data1, stopwords=russian_stopwords, amount=10):\n",
    "    print(i[0], i[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words={'тогда', 'свою', 'если', 'ж', 'этой', 'ей', 'после', 'хоть', 'опять', 'том', 'ни', 'не', 'все', 'даже', 'что', 'другой', 'его', 'бы', 'к', 'сам', 'нас', 'моя', 'тебя', 'им', 'ведь', 'вас', 'она', 'когда', 'три', 'на', 'хорошо', 'раз', 'потому', 'в', 'тут', 'здесь', 'я', 'нибудь', 'ее', '...оже', 'сейчас', 'их', 'при', 'будет', 'куда', 'этом', 'него', 'вам', 'ну', 'а', 'они', 'чтоб', 'во'},\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range = (1,1), stop_words=russian_stopwords)\n",
    "tfidf.fit([i[1] for i in qa_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_vectors_tf(lemmas, model, tfidf): \n",
    "    \n",
    "    vocab = tfidf.vocabulary_\n",
    "    arr = tfidf.transform([' '.join(lemmas)]).toarray()[0]\n",
    "    weight = 1\n",
    "    lemmas_vectors = []\n",
    "    all_weight = 0\n",
    "\n",
    "    for lemma in lemmas:\n",
    "        if lemma in vocab:\n",
    "            weight = arr[vocab[lemma]]\n",
    "        try:\n",
    "            lemmas_vectors.append(model.wv[lemma] * weight)\n",
    "            all_weight += weight\n",
    "        except:\n",
    "            None\n",
    "    if lemmas_vectors:\n",
    "        doc_vec = sum(lemmas_vectors) #/ all_weight\n",
    "        #doc_vec = list(matutils.unitvec(doc_vec))\n",
    "        return doc_vec\n",
    "    else: \n",
    "        return [0] * 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_w2v_tf(string, model, data1, stopwords={}, amount=10, del_stop=True):\n",
    "    \n",
    "    if not isinstance(string, str):\n",
    "        raise ValueError('enter correct data')\n",
    "        \n",
    "    global tfidf\n",
    "    words = preprocessing(string, stopwords=stopwords, del_stopwords=del_stop, del_digit=True)\n",
    "    vec = get_w2v_vectors_tf(words, model, tfidf)\n",
    "    answer = culc_sim_score(data1, vec)\n",
    "    \n",
    "    for index, ans in enumerate(sorted(answer.items(), key=lambda x: x[1][1],  reverse=True)):\n",
    "        if index >= amount: break\n",
    "        yield (ans[0], ans[1][0], ans[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25072254335260113"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = 0\n",
    "for index, qa in enumerate(qa_corpus):\n",
    "    idx = str(index) + '.txt'\n",
    "    for i in search_w2v_tf(qa[0], model, data1, stopwords=russian_stopwords, del_stop=True, amount=5):\n",
    "        if i[0] == idx:\n",
    "            pred += 1\n",
    "pred / len(qa_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расчет среднего арифметического оказался лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.txt 3.0969675\n",
      "315.txt 3.0812178\n",
      "402.txt 2.9510856\n",
      "622.txt 2.852767\n",
      "643.txt 2.4845889\n",
      "303.txt 2.4650326\n",
      "1259.txt 2.4629474\n",
      "817.txt 2.4600716\n",
      "1307.txt 2.4401164\n",
      "568.txt 2.4354715\n"
     ]
    }
   ],
   "source": [
    "for i in search_w2v_tf(qa_corpus[35][0], model, data1, stopwords=russian_stopwords, amount=10):\n",
    "    print(i[0], i[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество эпох у doc2vec (начинайте от 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagged_data_creator(main_dir, stopwords={}, del_stop=False):\n",
    "    \n",
    "    tagged_data = []\n",
    "    i = 0\n",
    "    for part in splitting(main_dir, stopwords=stopwords, del_stop=del_stop):\n",
    "        tagged_data.append(TaggedDocument(words=part[0], tags=[i]))\n",
    "        #print('Complited: ' + part[1])\n",
    "        i += 1\n",
    "    return tagged_data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doc2vec(tagged_data, epo=100):\n",
    "    \n",
    "    model = Doc2Vec(vector_size=100, min_count=5, alpha=0.025, \n",
    "                min_alpha=0.025, epochs=epo, workers=2, dm=1)\n",
    "    \n",
    "    print('building vocabulary')\n",
    "    model.build_vocab(tagged_data)\n",
    "    print('starting training...')\n",
    "    model.random.seed(42)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    print('model is trained')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_d2v_vectors(model, text):\n",
    "    \"\"\"Получает вектор документа\"\"\"\n",
    "    return model.infer_vector(text)\n",
    "    \n",
    "\n",
    "def save_d2v_base(main_dir, model, stopwords={}, del_stop=False):\n",
    "    \"\"\"Индексирует всю базу для поиска через doc2vec\"\"\"\n",
    "    \n",
    "    all_data = []\n",
    "\n",
    "    for part in splitting(main_dir, stopwords=stopwords, del_stop=del_stop):\n",
    "        vec_info = {}\n",
    "        vec_info['id'] = part[1]\n",
    "        vec_info['text'] = part[2]\n",
    "        vec_info['vec'] = get_d2v_vectors(model, part[0])\n",
    "        all_data.append(vec_info)\n",
    "    return all_data #json.dumps(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_d2v(string, model, data2, stopwords={}, del_stop=False, amount=10):\n",
    "    \n",
    "    if not isinstance(string, str):\n",
    "        raise ValueError('enter correct data')\n",
    "    \n",
    "    words = preprocessing(string, stopwords=stopwords, del_stopwords=del_stop, del_digit=True)\n",
    "    vec = get_d2v_vectors(model, words)\n",
    "    answer = culc_sim_score(data2, vec)\n",
    "    \n",
    "    for index, ans in enumerate(sorted(answer.items(), key=lambda x: x[1][1], reverse=True)):\n",
    "        if index >= amount: break\n",
    "        yield (ans[0], ans[1][0], ans[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir2 = '/Users/Stoneberry/Desktop/Uni/Прога/4 курс/Поиск/лекции/train'\n",
    "model_data = tagged_data_creator(dir2, stopwords=russian_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 100\n",
      "building vocabulary\n",
      "starting training...\n",
      "model is trained\n",
      "Starting: 200\n",
      "building vocabulary\n",
      "starting training...\n",
      "model is trained\n",
      "Starting: 300\n",
      "building vocabulary\n",
      "starting training...\n",
      "model is trained\n",
      "Starting: 400\n",
      "building vocabulary\n",
      "starting training...\n",
      "model is trained\n"
     ]
    }
   ],
   "source": [
    "for epo in range(100, 500, 100):\n",
    "\n",
    "    print('Starting: ' + str(epo))\n",
    "    my_model = train_doc2vec(model_data, epo=epo)\n",
    "    data2 = save_d2v_base(main_dir, my_model, stopwords=russian_stopwords, del_stop=True)\n",
    "\n",
    "    pred = 0\n",
    "    for index, qa in enumerate(qa_corpus):\n",
    "        idx = str(index) + '.txt'\n",
    "        for i in search_d2v(qa[0], my_model, data2, stopwords=russian_stopwords, del_stop=True, amount=5):\n",
    "            if i[0] == idx:\n",
    "                pred += 1\n",
    "    res.append(pred / len(qa_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f49b9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(100, 500, 100)), res)\n",
    "plt.ylabel('some numbers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший результат на 200 эпохах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработка документов для обучения doc2vec (удалять / не удалять стоп-слова)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не удаляя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building vocabulary\n",
      "starting training...\n",
      "model is trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1791907514450867"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = train_doc2vec(model_data, epo=100)\n",
    "data2 = save_d2v_base(main_dir, my_model, stopwords=russian_stopwords, del_stop=False)\n",
    "\n",
    "pred = 0\n",
    "for index, qa in enumerate(qa_corpus):\n",
    "    idx = str(index) + '.txt'\n",
    "    for i in search_d2v(qa[0], my_model, data2, stopwords=russian_stopwords, del_stop=False, amount=5):\n",
    "        if i[0] == idx:\n",
    "            pred += 1\n",
    "pred / len(qa_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаляя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1893063583815029"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = save_d2v_base(main_dir, my_model, stopwords=russian_stopwords, del_stop=True)\n",
    "pred = 0\n",
    "\n",
    "for index, qa in enumerate(qa_corpus):\n",
    "    idx = str(index) + '.txt'\n",
    "    for i in search_d2v(qa[0], my_model, data2, stopwords=russian_stopwords, del_stop=True, amount=5):\n",
    "        if i[0] == idx:\n",
    "            pred += 1\n",
    "pred / len(qa_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, удаляя стоп слова, accuracy выше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d2v & w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Блендинг методов поиска: соединить результаты обратного индекса и w2v, или (что проще) w2v и d2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging(w2v, d2v, all_):\n",
    "    \n",
    "    ans = {}\n",
    "\n",
    "    for item in all_:\n",
    "\n",
    "        if item in w2v: \n",
    "            it_w = w2v[item]\n",
    "        else: it_w = 0\n",
    "\n",
    "        if item in d2v: \n",
    "            it_d = d2v[item]\n",
    "        else: it_d = 0\n",
    "        \n",
    "        ans[item] = (it_w * 0.7 + it_d * 0.3) / 2\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serach_w2_d2(string, model, my_model, data1, data2, stopwords={}, del_stop=False, amount=10):\n",
    "\n",
    "    w2v = {i[0]:i[2] for i in search_w2v(string, model, data1, stopwords=stopwords, del_stop=del_stop, amount=amount)}\n",
    "    d2v = {i[0]:i[2] for i in search_d2v(string, my_model, data2, stopwords=stopwords, del_stop=del_stop, amount=amount)}\n",
    "    all_ = set(w2v) | set(d2v)\n",
    "    ans = merging(w2v, d2v, all_)\n",
    "    return sorted(ans.items(), reverse=True, key=lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3179190751445087"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = 0\n",
    "\n",
    "for index, qa in enumerate(qa_corpus):\n",
    "    idx = str(index) + '.txt'\n",
    "    for i in serach_w2_d2(qa[0], model, my_model, data1, data2,del_stop=True, stopwords=russian_stopwords, amount=5):\n",
    "        if i[0] == idx:\n",
    "            pred += 1\n",
    "pred / len(qa_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование двух моделей дает лучший результат"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter(main_dir, del_stop=True, stopwords={}):\n",
    "\n",
    "    word_count = defaultdict(dict) # word : {id, count}\n",
    "    id_text = defaultdict(list) # id : [len, text]\n",
    "\n",
    "    for root, dirs, files in os.walk(main_dir):\n",
    "        for name in files:\n",
    "            if not '.DS_Store' in name:\n",
    "                with open(os.path.join(root, name), 'r', encoding='utf-8') as f:\n",
    "                    text = f.read() \n",
    "                    words = preprocessing(text, stopwords=stopwords, del_stopwords=del_stop, del_digit=True)\n",
    "                    id_text[name] += [len(words), text]\n",
    "                    prob = Counter(words)\n",
    "                    for word in prob:\n",
    "                        word_count[word][name] = prob[word]\n",
    "                \n",
    "    return word_count, id_text\n",
    "\n",
    "\n",
    "def score_BM25(qf, dl, avgdl, k1, b, N, n):\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "\n",
    "    qf - кол - во вхождений слова в документе\n",
    "    dl - длина документа\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    tf = qf / dl\n",
    "    idf = log(N - n + 0.5 / n + 0.5)\n",
    "    a = (k1 + 1) * tf\n",
    "    b = tf + k1*(1 - b + b*(dl / avgdl))\n",
    "\n",
    "    return (a / b) * idf\n",
    "\n",
    "\n",
    "def compute_sim(text, doc, id_text, word_count, N, stopwords={}, del_stop=True):\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError('enter correct data')\n",
    "    if not isinstance(doc, str):\n",
    "        raise ValueError('enter correct data')\n",
    "\n",
    "    opr = [' ', '  ', '\\t', '\\n']\n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    avgdl = np.mean([i[0] for i in id_text.values()])\n",
    "\n",
    "    ans = 0\n",
    "\n",
    "    words = preprocessing(text, stopwords=stopwords, del_stopwords=del_stop, del_digit=True)\n",
    "\n",
    "    for word in words:\n",
    "        if word in word_count:\n",
    "            if doc in word_count[word]:\n",
    "                qf = word_count[word][doc]\n",
    "            else:\n",
    "                qf = 0\n",
    "            dl = id_text[doc][0]\n",
    "            n = len(word_count[word])\n",
    "            ans += score_BM25(qf, dl, avgdl, k1, b, N, n)\n",
    "\n",
    "    return ans\n",
    "\n",
    "\n",
    "def get_search_result(text, id_text, word_count, stopwords={}, del_stop=True, amount=10):\n",
    "    \"\"\"\n",
    "    Compute sim score between search query and all documents in collection\n",
    "    Collect as pair (doc_id, score)\n",
    "    :param query: input text\n",
    "    :return: list of lists with (doc_id, score)\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError\n",
    "\n",
    "    arr = []\n",
    "    N = len(id_text)\n",
    "   \n",
    "    for doc in id_text:\n",
    "        arr.append((doc, id_text[doc][1], compute_sim(text, doc, id_text, word_count, N, stopwords=stopwords, del_stop=del_stop)))\n",
    "    \n",
    "    arr = sorted(arr, key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    return arr[:amount]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count, id_text = counter(main_dir, stopwords=russian_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "687.txt 3.55123096620198\n",
      "329.txt 2.0828200173558153\n",
      "1008.txt 1.5150265255600737\n",
      "1176.txt 0.7071466707337445\n",
      "89.txt 0.5485908282723401\n"
     ]
    }
   ],
   "source": [
    "for i in get_search_result('приговор', id_text, word_count, stopwords=russian_stopwords, amount=5):\n",
    "    print(i[0], i[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Совмещение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging_all_3(w2v, d2v, okapi, all_):\n",
    "    \n",
    "    ans = {}\n",
    "\n",
    "    for item in all_:\n",
    "\n",
    "        if item in w2v: \n",
    "            it_w = w2v[item]\n",
    "        else: it_w = 0\n",
    "\n",
    "        if item in d2v: \n",
    "            it_d = d2v[item]\n",
    "        else: it_d = 0\n",
    "            \n",
    "        if item in okapi:\n",
    "            it_o = okapi[item]\n",
    "        else: it_o = 0\n",
    "        \n",
    "        ans[item] = (((it_w * 0.8 + it_o * 0.2) / 2) * 0.7 + it_d * 0.3) / 2\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serach_w2_d2_ok(string, model, my_model, data1, data2, word_count, id_text, stopwords={}, del_stop=False, amount=10):\n",
    "\n",
    "    w2v = {i[0]:i[2] for i in search_w2v(string, model, data1, stopwords=stopwords, del_stop=del_stop, amount=amount)}\n",
    "    d2v = {i[0]:i[2] for i in search_d2v(string, my_model, data2, stopwords=stopwords, del_stop=del_stop, amount=amount)}\n",
    "    okapi = {i[0]:i[2] for i in get_search_result(string, id_text, word_count, stopwords=russian_stopwords, del_stop=del_stop, amount=amount)}\n",
    "    all_ = set(w2v) | set(d2v) | set(okapi)\n",
    "    ans = merging_all_3(w2v, d2v, okapi, all_)\n",
    "    return sorted(ans.items(), reverse=True, key=lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-249-2364e825eafc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0ml\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mserach_w2_d2_ok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdel_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrussian_stopwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-230-6144b6d45f47>\u001b[0m in \u001b[0;36mserach_w2_d2_ok\u001b[0;34m(string, model, my_model, data1, data2, word_count, id_text, stopwords, del_stop, amount)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mw2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msearch_w2v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdel_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdel_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0md2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msearch_d2v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdel_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdel_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mokapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_search_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrussian_stopwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdel_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdel_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mall_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md2v\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mokapi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerging_all_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mokapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-223-88543a06af4c>\u001b[0m in \u001b[0;36mget_search_result\u001b[0;34m(text, id_text, word_count, stopwords, del_stop, amount)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mid_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdel_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdel_stop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-223-88543a06af4c>\u001b[0m in \u001b[0;36mcompute_sim\u001b[0;34m(text, doc, id_text, word_count, N, stopwords, del_stop)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdel_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdel_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdel_digit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-b2056ef0eb9b>\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m(input_text, stopwords, del_stopwords, del_digit)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'»«–…'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mlemmas_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlemmas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-b2056ef0eb9b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'»«–…'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mlemmas_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlemmas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/pymystem3/mystem.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mneed_encode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/pymystem3/mystem.py\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_analyze_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/pymystem3/mystem.py\u001b[0m in \u001b[0;36m_analyze_impl\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0mselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_procout_no\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pred = 0\n",
    "l = 0\n",
    "for index, qa in enumerate(qa_corpus):\n",
    "    print(index)\n",
    "    l += 1\n",
    "    idx = str(index) + '.txt'\n",
    "    for i in serach_w2_d2_ok(qa[0], model, my_model, data1, data2, word_count, id_text, del_stop=True, stopwords=russian_stopwords, amount=5):\n",
    "        if i[0] == idx:\n",
    "            pred += 1\n",
    "pred / len(qa_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не успела досчитать до дедлайна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36942675159235666"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred/l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На маленькой части текстов получается самый высокий результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
