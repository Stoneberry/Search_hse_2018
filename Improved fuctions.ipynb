{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def score_BM25(qf, dl, avgdl, k1, b, N, n):\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "\n",
    "    qf - кол - во вхождений слова в документе\n",
    "    dl - длина документа\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    tf = qf / dl\n",
    "    idf = log(N - n + 0.5 / n + 0.5)\n",
    "    a = (k1 + 1) * tf\n",
    "    b = tf + k1*(1 - b + b*(dl / avgdl))\n",
    "\n",
    "    return (a / b) * idf\n",
    "\n",
    "\n",
    "def compute_sim(words, doc, info_data, word_count, N, avgdl):\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "\n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    ans = 0\n",
    "\n",
    "    for word in words:\n",
    "        if word_count[word] != {}:\n",
    "\n",
    "            try: qf = word_count[word][doc]\n",
    "            except KeyError: qf = 0\n",
    "\n",
    "            dl = info_data[doc]['len']\n",
    "            n = len(word_count[word])\n",
    "            ans += score_BM25(qf, dl, avgdl, k1, b, N, n)\n",
    "\n",
    "    return ans\n",
    "\n",
    "\n",
    "def get_search_result(text, info_data, word_count, stopwords={}, del_stop=True, amount=10):\n",
    "    \"\"\"\n",
    "    Compute sim score between search query and all documents in collection\n",
    "    Collect as pair (doc_id, score)\n",
    "    :param query: input text\n",
    "    :return: list of lists with (doc_id, score)\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError\n",
    "    \n",
    "    words = preprocessing(text, stopwords=stopwords, del_stopwords=del_stop, del_digit=True)\n",
    "    answer = {}\n",
    "    N = len(info_data)\n",
    "   \n",
    "    for doc in info_data:\n",
    "        answer[doc] = compute_sim(words, doc, info_data, word_count, N, avgdl)\n",
    "\n",
    "    for index, ans in enumerate(sorted(answer.items(), reverse=True, key=lambda x: x[1])):\n",
    "        if index >= amount: break\n",
    "        yield (ans[0], info_data[ans[0]], ans[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_vectors(lemmas, model):\n",
    "\n",
    "    lemmas_vectors = []\n",
    "    for lemma in lemmas:\n",
    "        try:\n",
    "            lemmas_vectors.append(model.wv[lemma])\n",
    "        except:\n",
    "            None\n",
    "    if lemmas_vectors:\n",
    "        doc_vec = sum(lemmas_vectors)\n",
    "        normalized_vec = matutils.unitvec(doc_vec)\n",
    "        return list(normalized_vec)\n",
    "    else:\n",
    "        return [0] * 300\n",
    "\n",
    "\n",
    "def similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2)\n",
    "\n",
    "\n",
    "def culc_sim_score(all_data, vec, model_type):\n",
    "\n",
    "    answer = defaultdict(float)  # id : score\n",
    "\n",
    "    for part in all_data:\n",
    "\n",
    "        if model_type == 'word2v':\n",
    "            sim = similarity(part['w2v_vec'], vec)\n",
    "        elif model_type == 'doc2v':\n",
    "            sim = similarity(part['d2v_vec'], vec)\n",
    "        else: raise ValueError\n",
    "\n",
    "        if answer[part['id']] == 0.0: answer[part['id']] = float('-inf')\n",
    "\n",
    "        if sim > answer[part['id']]: answer[part['id']] = sim\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "def search_w2v(string, model, info_data, vec_data, stopwords={}, amount=10, del_stop=True):\n",
    "\n",
    "    if not isinstance(string, str):\n",
    "        raise ValueError('enter correct data')\n",
    "\n",
    "    words = preprocessing(string, stopwords=stopwords, del_stopwords=del_stop, del_digit=True)\n",
    "    vec = get_w2v_vectors(words, model)\n",
    "    answer = culc_sim_score(vec_data, vec, 'word2v')\n",
    "\n",
    "    for index, ans in enumerate(sorted(answer.items(), reverse=True, key=lambda x: x[1])):\n",
    "        if index >= amount: break\n",
    "        yield (ans[0], info_data[ans[0]], ans[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_d2v_vectors(text, model):\n",
    "    \"\"\"Получает вектор документа\"\"\"\n",
    "    return model.infer_vector(text)\n",
    "\n",
    "\n",
    "def search_d2v(string, model, info_data, vec_data, stopwords={}, del_stop=False, amount=10):\n",
    "\n",
    "    if not isinstance(string, str):\n",
    "        raise ValueError('enter correct data')\n",
    "\n",
    "    words = preprocessing(string, stopwords=stopwords, del_stopwords=del_stop, del_digit=True)\n",
    "    vec = get_d2v_vectors(words, model)\n",
    "    answer = culc_sim_score(vec_data, vec, 'doc2v')\n",
    "\n",
    "    for index, ans in enumerate(sorted(answer.items(), reverse=True, key=lambda x: x[1])):\n",
    "        if index >= amount: break\n",
    "        yield (ans[0], info_data[ans[0]], ans[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging_all_3(w2v, d2v, okapi, all_):\n",
    "\n",
    "    ans = {}\n",
    "\n",
    "    for item in all_:\n",
    "\n",
    "        try: it_w = w2v[item][1]\n",
    "        except KeyError: it_w = 0\n",
    "\n",
    "        try: it_d = d2v[item][1]\n",
    "        except KeyError: it_d = 0\n",
    "\n",
    "        try: it_o = okapi[item][1]\n",
    "        except KeyError: it_o = 0\n",
    "\n",
    "        score = ((it_o * 0.8) + ((it_d * 0.2 + it_w + 0.8) / 2)*0.2) / 2\n",
    "        ans[item] = score\n",
    "    return ans\n",
    "\n",
    "\n",
    "def search_w2_d2_ok(string, avgdl, model_w2v, model_d2v, info_data, vec_data, word_count, stopwords={}, del_stop=False, amount=10):\n",
    "\n",
    "    w2v = {i[0]: (i[1], i[2]) for i in search_w2v(string, model_w2v, info_data, vec_data, stopwords=stopwords, amount=amount, del_stop=del_stop)}\n",
    "    d2v = {i[0]: (i[1], i[2]) for i in search_d2v(string, model_d2v, info_data, vec_data, stopwords=stopwords, amount=amount, del_stop=del_stop)}\n",
    "    okapi = {i[0]: (i[1], i[2]) for i in get_search_result(string, avgdl, info_data, word_count, stopwords=stopwords, del_stop=del_stop, amount=amount)}\n",
    "\n",
    "    all_ = set(w2v.keys()) | set(d2v.keys()) | set(okapi.keys())\n",
    "    answer = merging_all_3(w2v, d2v, okapi, all_)\n",
    "\n",
    "    for index, ans in enumerate(sorted(answer.items(), reverse=True, key=lambda x: x[1])):\n",
    "        if index >= amount: break\n",
    "        yield (ans[0], info_data[ans[0]], ans[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search(string, search_method, avgdl, model_w2v, model_d2v, info_data, vec_data_del, vec_data_not_del, word_count_del, word_count_not_del, amount=10, del_stop=True, stopwords={}):\n",
    "\n",
    "    if search_method == 'inverted_index':\n",
    "        if del_stop != 'True':\n",
    "            search_result = get_search_result(string, avgdl, info_data, word_count_not_del, stopwords=stopwords, del_stop=False, amount=amount)\n",
    "        else:\n",
    "            search_result = get_search_result(string, avgdl, info_data, word_count_del, stopwords=stopwords, del_stop=True, amount=amount)\n",
    "\n",
    "    elif search_method == 'word2vec':\n",
    "        if del_stop != 'True':\n",
    "            search_result = search_w2v(string, model_w2v, info_data, vec_data_not_del, stopwords=stopwords, amount=amount, del_stop=False)\n",
    "        else:\n",
    "            search_result = search_w2v(string, model_w2v, info_data, vec_data_del, stopwords=stopwords, amount=amount, del_stop=True)\n",
    "\n",
    "    elif search_method == 'doc2vec':\n",
    "        if del_stop != 'True':\n",
    "            search_result = search_d2v(string, model_d2v, info_data, vec_data_not_del, stopwords=stopwords, amount=amount, del_stop=False)\n",
    "        else:\n",
    "            search_result = search_d2v(string, model_d2v, info_data, vec_data_del, stopwords=stopwords, amount=amount, del_stop=True)\n",
    "\n",
    "    elif search_method == 'all':\n",
    "        if del_stop != 'True':\n",
    "            search_result = search_w2_d2_ok(string, avgdl, model_w2v, model_d2v, info_data, vec_data_not_del, word_count_not_del, del_stop=False, stopwords=stopwords, amount=amount)\n",
    "        else:\n",
    "            search_result = search_w2_d2_ok(string, avgdl, model_w2v, model_d2v, info_data, vec_data_del, word_count_del, del_stop=True, stopwords=stopwords, amount=amount)\n",
    "\n",
    "    else:\n",
    "        raise TypeError('unsupported search method')\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
