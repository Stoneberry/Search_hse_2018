{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import gensim\n",
    "from judicial_splitter import split_paragraph, get_sentences\n",
    "import string\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from gensim import matutils\n",
    "import numpy as np \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from math import log\n",
    "from collections import defaultdict, Counter\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_stopwords = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "База авито"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = '/Users/Stoneberry/Desktop/Uni/Прога/4 курс/Поиск/project/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Файл - сслыка на объявление"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_data_maker(main_dir, del_stop=True, stopwords={}):\n",
    "    \"\"\"\n",
    "    {название файла: {сслыка на объявление авито, title, text, len}}\n",
    "    \"\"\"\n",
    "    all_data = defaultdict(dict) \n",
    "    word_count = defaultdict(dict) # word : {id: count}\n",
    "\n",
    "    for root, dirs, files in os.walk(main_dir):\n",
    "        for name in files:\n",
    "            if not '.DS_Store' in name:\n",
    "                with open(os.path.join(root, name), 'r', encoding='utf-8') as f:\n",
    "                    text_lines = f.readlines()\n",
    "                    text = re.sub('\\n', '', (' '.join(text_lines[:-2])))\n",
    "                    words = preprocessing(text, stopwords=stopwords, del_stopwords=del_stop, del_digit=True)\n",
    "                    all_data[name] = {\"link\": text_lines[-1],\n",
    "                                      'title': text_lines[-2], \n",
    "                                      'text': text,\n",
    "                                      'len': len(words)}\n",
    "                    prob = Counter(words)\n",
    "                    for word in prob:\n",
    "                        word_count[word][name] = prob[word]\n",
    "    return all_data, word_count\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(input_text, stopwords={}, del_stopwords=True, del_digit=True):\n",
    "    \n",
    "    words = [x.lower().strip(string.punctuation+'»«–…') for x in word_tokenize(input_text)]\n",
    "    lemmas = [mystem.lemmatize(x)[0] for x in words if x]\n",
    "    lemmas_arr = []\n",
    "    for lemma in lemmas:\n",
    "        if del_stopwords:\n",
    "            if lemma in stopwords:\n",
    "                continue\n",
    "        if del_digit:\n",
    "            if lemma.isdigit():\n",
    "                continue\n",
    "        lemmas_arr.append(lemma)\n",
    "    return lemmas_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting(main_dir, stopwords={}, del_stop=True):\n",
    "\n",
    "    for root, dirs, files in os.walk(main_dir):\n",
    "        for name in files:\n",
    "            if not '.DS_Store' in name:\n",
    "                with open(os.path.join(root, name), 'r', encoding='utf-8') as f:\n",
    "                    text = f.read() \n",
    "                    parts = split_paragraph(get_sentences(text), 4)\n",
    "                    for part in parts:\n",
    "                        clean_part = preprocessing(part, stopwords, del_stopwords=del_stop, del_digit=True)\n",
    "                        yield (clean_part, name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем данные в один массив"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_base(main_dir, model_w2v, model_d2v, stopwords={}, del_stop=True):\n",
    "    \"\"\"Индексирует всю базу для поиска\n",
    "    [{id, w2v, d2v}]\n",
    "    \"\"\"\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    for part in splitting(main_dir, stopwords=stopwords, del_stop=del_stop):\n",
    "        vec_info = {}\n",
    "        vec_info['id'] = part[1]\n",
    "        vec_info['w2v_vec'] = get_w2v_vectors(part[0], model_w2v)\n",
    "        vec_info['d2v_vec'] = get_d2v_vectors(part[0], model_d2v)\n",
    "        all_data.append(vec_info)\n",
    "\n",
    "    return all_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_vectors(lemmas, model): \n",
    "\n",
    "    lemmas_vectors = []\n",
    "    for lemma in lemmas:\n",
    "        try:\n",
    "            lemmas_vectors.append(model.wv[lemma])\n",
    "        except:\n",
    "            None\n",
    "    if lemmas_vectors:\n",
    "        doc_vec = sum(lemmas_vectors)\n",
    "        normalized_vec = matutils.unitvec(doc_vec)\n",
    "        return list(normalized_vec)\n",
    "    else: \n",
    "        return [0] * 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(v1, v2):\n",
    "    v1_norm = matutils.unitvec(np.array(v1))\n",
    "    v2_norm = matutils.unitvec(np.array(v2))\n",
    "    return np.dot(v1_norm, v2_norm)\n",
    "\n",
    "\n",
    "def similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2)\n",
    "\n",
    "\n",
    "def culc_sim_score(all_data, vec, model_type):\n",
    "\n",
    "    answer = defaultdict(float)  # id : score\n",
    "    \n",
    "    for part in all_data:\n",
    "\n",
    "        if model_type == 'word2v':\n",
    "            sim = similarity(part['w2v_vec'], vec)\n",
    "        elif model_type == 'doc2v':\n",
    "            sim = similarity(part['d2v_vec'], vec)\n",
    "        else: raise ValueError\n",
    "            \n",
    "        if answer[part['id']] == 0.0: answer[part['id']] = float('-inf')\n",
    "    \n",
    "        if sim > answer[part['id']]: answer[part['id']] = sim\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_w2v(string, model, info_data, vec_data, stopwords={}, amount=10, del_stop=True):\n",
    "    \n",
    "    if not isinstance(string, str):\n",
    "        raise ValueError('enter correct data')\n",
    "    \n",
    "    words = preprocessing(string, stopwords=stopwords, del_stopwords=del_stop, del_digit=True)\n",
    "    vec = get_w2v_vectors(words, model)\n",
    "    answer = culc_sim_score(vec_data, vec, 'word2v')\n",
    "    \n",
    "    for index, ans in enumerate(sorted(answer.items(), reverse=True, key=lambda x: x[1])):\n",
    "        if index >= amount: break\n",
    "        yield (ans[0], info_data[ans[0]], ans[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagged_data_creator(main_dir, stopwords={}, del_stop=False):\n",
    "    \n",
    "    tagged_data = []\n",
    "    i = 0\n",
    "    for part in splitting(main_dir, stopwords=stopwords, del_stop=del_stop):\n",
    "        tagged_data.append(TaggedDocument(words=part[0], tags=[i]))\n",
    "        i += 1\n",
    "    return tagged_data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doc2vec(tagged_data, epo=100):\n",
    "    \n",
    "    model = Doc2Vec(vector_size=100, min_count=5, alpha=0.025, \n",
    "                min_alpha=0.025, epochs=epo, workers=4, dm=1)\n",
    "    \n",
    "    print('building vocabulary')\n",
    "    model.build_vocab(tagged_data)\n",
    "    print('starting training...')\n",
    "    model.random.seed(42)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    print('model is trained')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_d2v_vectors(text, model):\n",
    "    \"\"\"Получает вектор документа\"\"\"\n",
    "    return model.infer_vector(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_d2v(string, model, info_data, vec_data, stopwords={}, del_stop=False, amount=10):\n",
    "    \n",
    "    if not isinstance(string, str):\n",
    "        raise ValueError('enter correct data')\n",
    "    \n",
    "    words = preprocessing(string, stopwords=stopwords, del_stopwords=del_stop, del_digit=True)\n",
    "    vec = get_d2v_vectors(words, model)\n",
    "    answer = culc_sim_score(vec_data, vec, 'doc2v')\n",
    "    \n",
    "    for index, ans in enumerate(sorted(answer.items(), reverse=True, key=lambda x: x[1])):\n",
    "        if index >= amount: break\n",
    "        yield (ans[0], info_data[ans[0]], ans[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем все в одном месте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = Word2Vec.load('/Users/Stoneberry/Desktop/Uni/Прога/4 курс/Поиск/araneum_none_fasttextcbow_300_5_2018/araneum_none_fasttextcbow_300_5_2018.model')\n",
    "model_d2v = Doc2Vec.load('/Users/Stoneberry/Desktop/Uni/Прога/4 курс/Поиск/my_google/my_d2v_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_data, word_count_del = all_data_maker(main_dir, del_stop=True, stopwords=russian_stopwords)\n",
    "info_data, word_count_not_del = all_data_maker(main_dir, del_stop=False, stopwords=russian_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_data_del = save_base(main_dir, model_w2v, model_d2v, del_stop=True, stopwords=russian_stopwords)\n",
    "vec_data_not_del = save_base(main_dir, model_w2v, model_d2v, del_stop=False, stopwords=russian_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('info_data.pickle', 'wb') as f:\n",
    "    pickle.dump(info_data, f)\n",
    "with open('word_count_del.pickle', 'wb') as f:\n",
    "    pickle.dump(word_count_del, f)\n",
    "with open('word_count_not_del.pickle', 'wb') as f:\n",
    "    pickle.dump(word_count_not_del, f)\n",
    "with open('vec_data_del.pickle', 'wb') as f:\n",
    "    pickle.dump(vec_data_del, f)\n",
    "with open('vec_data_not_del.pickle', 'wb') as f:\n",
    "    pickle.dump(vec_data_not_del, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgdl = np.mean([i['len'] for i in info_data.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('info_data.pickle', 'rb') as f:\n",
    "    info_data = pickle.load(f)\n",
    "with open('word_count_del.pickle', 'rb') as f:\n",
    "    word_count_del = pickle.load(f)\n",
    "with open('word_count_not_del.pickle', 'rb') as f:\n",
    "    word_count_not_del = pickle.load(f)\n",
    "with open('vec_data_del.pickle', 'rb') as f:\n",
    "    vec_data_del = pickle.load(f)\n",
    "with open('vec_data_not_del.pickle', 'rb') as f:\n",
    "    vec_data_not_del = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def score_BM25(qf, dl, avgdl, k1, b, N, n):\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "\n",
    "    qf - кол - во вхождений слова в документе\n",
    "    dl - длина документа\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    tf = qf / dl\n",
    "    idf = log(N - n + 0.5 / n + 0.5)\n",
    "    a = (k1 + 1) * tf\n",
    "    b = tf + k1*(1 - b + b*(dl / avgdl))\n",
    "\n",
    "    return (a / b) * idf\n",
    "\n",
    "\n",
    "def compute_sim(words, doc, info_data, word_count, N, avgdl):\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "\n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    ans = 0\n",
    "\n",
    "    for word in words:\n",
    "        if word_count[word] != {}:\n",
    "\n",
    "            try: qf = word_count[word][doc]\n",
    "            except KeyError: qf = 0\n",
    "\n",
    "            dl = info_data[doc]['len']\n",
    "            n = len(word_count[word])\n",
    "            ans += score_BM25(qf, dl, avgdl, k1, b, N, n)\n",
    "\n",
    "    return ans\n",
    "\n",
    "\n",
    "def get_search_result(text, info_data, word_count, stopwords={}, del_stop=True, amount=10):\n",
    "    \"\"\"\n",
    "    Compute sim score between search query and all documents in collection\n",
    "    Collect as pair (doc_id, score)\n",
    "    :param query: input text\n",
    "    :return: list of lists with (doc_id, score)\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError\n",
    "    \n",
    "    words = preprocessing(text, stopwords=stopwords, del_stopwords=del_stop, del_digit=True)\n",
    "    answer = {}\n",
    "    N = len(info_data)\n",
    "   \n",
    "    for doc in info_data:\n",
    "        answer[doc] = compute_sim(words, doc, info_data, word_count, N, avgdl)\n",
    "\n",
    "    for index, ans in enumerate(sorted(answer.items(), reverse=True, key=lambda x: x[1])):\n",
    "        if index >= amount: break\n",
    "        yield (ans[0], info_data[ans[0]], ans[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging_all_3(w2v, d2v, okapi, all_):\n",
    "    \n",
    "    ans = {}\n",
    "\n",
    "    for item in all_:\n",
    "\n",
    "        try: it_w = w2v[item][1]\n",
    "        except KeyError: it_w = 0\n",
    "\n",
    "        try: it_d = d2v[item][1]\n",
    "        except KeyError: it_d = 0\n",
    "            \n",
    "        try: it_o = okapi[item][1]\n",
    "        except KeyError: it_o = 0\n",
    "\n",
    "        score = ((it_o * 0.8) + ((it_d * 0.2 + it_w + 0.8) / 2)*0.2) / 2\n",
    "        ans[item] = score\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serach_w2_d2_ok(string, model_w2v, model_d2v, info_data, vec_data, word_count, stopwords={}, del_stop=False, amount=10):\n",
    "\n",
    "    w2v = {i[0]:(i[1], i[2]) for i in search_w2v(string, model_w2v, info_data, vec_data, stopwords=stopwords, amount=amount, del_stop=del_stop)}\n",
    "    d2v = {i[0]:(i[1], i[2]) for i in search_d2v(string, model_d2v, info_data, vec_data, stopwords=stopwords, amount=amount, del_stop=del_stop)}\n",
    "    okapi = {i[0]:(i[1], i[2]) for i in get_search_result(string,  info_data, word_count, stopwords=stopwords, del_stop=del_stop, amount=amount)}\n",
    "    \n",
    "    all_ = set(w2v.keys()) | set(d2v.keys()) | set(okapi.keys())\n",
    "    answer = merging_all_3(w2v, d2v, okapi, all_)\n",
    "    \n",
    "    for index, ans in enumerate(sorted(answer.items(), reverse=True, key=lambda x: x[1])):\n",
    "        if index >= amount: break\n",
    "        yield (ans[0], info_data[ans[0]], ans[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(string, search_method, model_w2v, model_d2v, info_data, vec_data_del, vec_data_not_del, word_count_del, word_count_not_del, amount=10, del_stop=True, stopwords={}):\n",
    "        \n",
    "    if search_method == 'inverted_index':\n",
    "        if del_stop is not True:\n",
    "            search_result = ((i) for i in get_search_result(string, info_data, word_count_not_del, stopwords=stopwords, del_stop=del_stop, amount=amount))\n",
    "        else:\n",
    "            search_result = (i for i inget_search_result(string, info_data, word_count_del, stopwords=stopwords, del_stop=del_stop, amount=amount))\n",
    "\n",
    "    elif search_method == 'word2vec':\n",
    "        if del_stop is not True:\n",
    "            search_result = (i for i in search_w2v(string, model_w2v, info_data, vec_data_not_del, stopwords=stopwords, amount=amount, del_stop=del_stop))\n",
    "        else:\n",
    "            search_result = (i for i in search_w2v(string, model_w2v, info_data, vec_data_del, stopwords=stopwords, amount=amount, del_stop=del_stop))\n",
    "\n",
    "    elif search_method == 'doc2vec':\n",
    "        if del_stop is not True:\n",
    "            search_result = (i for i in search_d2v(string, model_d2v, info_data, vec_data_not_del, stopwords=stopwords, amount=amount, del_stop=del_stop))\n",
    "        else:\n",
    "            search_result = (i for i in search_d2v(string, model_d2v, info_data, vec_data_del, stopwords=stopwords, amount=amount, del_stop=del_stop))\n",
    "    \n",
    "    elif search_method == 'all':\n",
    "        if del_stop is not True:\n",
    "            search_result =  (i for i in serach_w2_d2_ok(string, model_w2v, model_d2v, info_data, vec_data_not_del, word_count_not_del, del_stop=del_stop, stopwords=stopwords, amount=amount))\n",
    "        else:\n",
    "            search_result =  (i for i in serach_w2_d2_ok(string, model_w2v, model_d2v, info_data, vec_data_del, word_count_del, del_stop=del_stop, stopwords=stopwords, amount=amount))\n",
    "    \n",
    "    else:\n",
    "        raise TypeError('unsupported search method')\n",
    "    \n",
    "    return search_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
